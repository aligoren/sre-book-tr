# BÃ¶lÃ¼m 2 - Google'da Production Environment, Bir SRE'nin BakÄ±ÅŸ AÃ§Ä±sÄ±ndan

**Yazan:** JC van Winkel  
**EditÃ¶r:** Betsy Beyer

Google datacenterlarÄ± Ã§oÄŸu geleneksel datacenter ve kÃ¼Ã§Ã¼k Ã¶lÃ§ekli server farm'larÄ±ndan Ã§ok farklÄ±dÄ±r. Bu farklÄ±lÄ±klar hem ekstra problemler hem de fÄ±rsatlar sunar. Bu bÃ¶lÃ¼m Google datacenterlarÄ±nÄ± karakterize eden zorluklarÄ± ve fÄ±rsatlarÄ± tartÄ±ÅŸÄ±r ve kitap boyunca kullanÄ±lan terminolojiyi tanÄ±tÄ±r.

## Hardware

Google'Ä±n compute kaynaklarÄ±nÄ±n Ã§oÄŸu, Google tarafÄ±ndan tasarlanan datacenterlarda bulunan Ã¶zel power distribution, cooling, networking ve compute hardware'Ä± ile bulunur. "Standart" colocation datacenterlarÄ±n aksine, Google tasarÄ±mÄ± bir datacenterdaki compute hardware'Ä± tamamÄ± aynÄ±dÄ±r. Server hardware'Ä± ile server software'Ä± arasÄ±ndaki karÄ±ÅŸÄ±klÄ±ÄŸÄ± ortadan kaldÄ±rmak iÃ§in, kitap boyunca ÅŸu terminolojiyi kullanÄ±yoruz:

**Machine (Makine)**
Bir hardware parÃ§asÄ± (veya belki bir VM)

**Server**
Bir servisi uygulayan bir software parÃ§asÄ±

Makineler herhangi bir serveri Ã§alÄ±ÅŸtÄ±rabilir, bu yÃ¼zden belirli makineleri belirli server programlarÄ±na tahsis etmiyoruz. Ã–rneÄŸin mail sunucumuzu Ã§alÄ±ÅŸtÄ±ran belirli bir makine yoktur. Bunun yerine, kaynak tahsisi cluster iÅŸletim sistemimiz _Borg_ tarafÄ±ndan yÃ¶netilir.

Bu _server_ kelimesinin kullanÄ±mÄ±nÄ±n alÄ±ÅŸÄ±lmadÄ±k olduÄŸunu fark ediyoruz. Kelimenin yaygÄ±n kullanÄ±mÄ± "network baÄŸlantÄ±sÄ± kabul eden binary" ile _makineyi_ birbirine karÄ±ÅŸtÄ±rÄ±r, ancak ikisini ayÄ±rt etmek Google'da bilgisayarla Ã§alÄ±ÅŸÄ±rken Ã¶nemlidir. Bizim _server_ kullanÄ±mÄ±mÄ±za alÄ±ÅŸtÄ±ÄŸÄ±nÄ±zda, sadece Google iÃ§inde deÄŸil bu kitabÄ±n geri kalanÄ±nda da bu Ã¶zel terminolojiyi kullanmanÄ±n neden mantÄ±klÄ± olduÄŸu daha belirgin hale gelir.

Åekil 2-1 bir Google datacenter'Ä±nÄ±n topolojisini gÃ¶sterir:

* Onlarca makine bir _rack_'te yerleÅŸtirilir.
* Rack'ler bir _row_'da (sÄ±ra) durur.
* Bir veya daha fazla row bir _cluster_ oluÅŸturur.
* Genellikle bir _datacenter_ binasÄ± birden fazla cluster barÄ±ndÄ±rÄ±r.
* YakÄ±n yerlerde bulunan birden fazla datacenter binasÄ± bir _campus_ oluÅŸturur.

Belirli bir datacenter iÃ§indeki makinelerin birbirleriyle konuÅŸabilmesi gerekir, bu yÃ¼zden on binlerce portlu Ã§ok hÄ±zlÄ± bir virtual switch oluÅŸturduk. Bunu _Jupiter_ adlÄ± Clos network fabric'te yÃ¼zlerce Google yapÄ±mÄ± switch'i baÄŸlayarak baÅŸardÄ±k. En bÃ¼yÃ¼k konfigÃ¼rasyonunda Jupiter, serverlar arasÄ±nda 1.3 Pbps bisection bandwidth destekler.

Datacenterlar dÃ¼nya Ã§apÄ±ndaki backbone networkÃ¼mÃ¼z _B4_ ile birbirlerine baÄŸlanÄ±r. B4 bir software-defined networking mimarisidir (ve OpenFlow aÃ§Ä±k standart iletiÅŸim protokolÃ¼nÃ¼ kullanÄ±r). MÃ¼tevazÄ± sayÄ±da siteye bÃ¼yÃ¼k miktarda bandwidth saÄŸlar ve ortalama bandwidth'i maksimize etmek iÃ§in elastic bandwidth allocation kullanÄ±r.

# Hardware'Ä± "Organize Eden" System Software'Ä±

Hardware'Ä±mÄ±z bÃ¼yÃ¼k Ã¶lÃ§eÄŸi kaldÄ±rabilecek software tarafÄ±ndan kontrol edilmeli ve yÃ¶netilmelidir. Hardware arÄ±zalarÄ± software ile yÃ¶nettiÄŸimiz Ã¶nemli bir problemdir. Bir clusterdaki Ã§ok sayÄ±da hardware komponenti gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, hardware arÄ±zalarÄ± oldukÃ§a sÄ±k meydana gelir. Tipik bir yÄ±lda tek bir clusterde binlerce makine arÄ±zalanÄ±r ve binlerce hard disk bozulur; global olarak iÅŸlettiÄŸimiz cluster sayÄ±sÄ±yla Ã§arpÄ±ldÄ±ÄŸÄ±nda, bu sayÄ±lar biraz nefes kesici hale gelir. Bu nedenle, bu tÃ¼r problemleri kullanÄ±cÄ±lardan soyutlamak istiyoruz ve servislerimizi Ã§alÄ±ÅŸtÄ±ran takÄ±mlar da hardware arÄ±zalarÄ±yla uÄŸraÅŸmak istemez. Her datacenter campus'Ä±nda hardware ve datacenter altyapÄ±sÄ±nÄ±n bakÄ±mÄ±nÄ± yapmaya adanmÄ±ÅŸ takÄ±mlar vardÄ±r.

## Makineleri YÃ¶netmek

Åekil 2-2'de gÃ¶sterilen _Borg_, Apache Mesos'a benzer bir distributed cluster iÅŸletim sistemidir. Borg job'larÄ±nÄ± cluster seviyesinde yÃ¶netir.

Borg kullanÄ±cÄ±larÄ±n _job_'larÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmaktan sorumludur; bunlar sÃ¼resiz Ã§alÄ±ÅŸan serverlar veya MapReduce gibi batch iÅŸlemler olabilir. Job'lar gÃ¼venilirlik nedenleriyle ve tek bir process genellikle tÃ¼m cluster trafiÄŸini kaldÄ±ramadÄ±ÄŸÄ± iÃ§in birden fazla (bazen binlerce) Ã¶zdeÅŸ _task_'tan oluÅŸabilir. Borg bir job baÅŸlattÄ±ÄŸÄ±nda, task'lar iÃ§in makineler bulur ve makinelere server programÄ±nÄ± baÅŸlatmalarÄ±nÄ± sÃ¶yler. Borg daha sonra bu task'larÄ± sÃ¼rekli izler. EÄŸer bir task arÄ±zalanÄ±rsa, Ã¶ldÃ¼rÃ¼lÃ¼r ve muhtemelen farklÄ± bir makinede yeniden baÅŸlatÄ±lÄ±r.

Task'lar makineler Ã¼zerinde akÄ±ÅŸkan olarak tahsis edildiÄŸinden, task'lara baÅŸvurmak iÃ§in basitÃ§e IP adreslerine ve port numaralarÄ±na gÃ¼venemeyiz. Bu problemi ek bir indirection seviyesiyle Ã§Ã¶zÃ¼yoruz: bir job baÅŸlatÄ±rken, Borg her task'a _Borg Naming Service_ (BNS) kullanarak bir isim ve index numarasÄ± tahsis eder. IP adresi ve port numarasÄ± kullanmak yerine, diÄŸer processler Borg task'larÄ±na BNS ismi aracÄ±lÄ±ÄŸÄ±yla baÄŸlanÄ±r, bu da BNS tarafÄ±ndan IP adresi ve port numarasÄ±na Ã§evrilir. Ã–rneÄŸin, BNS path'i `/bns/<cluster>/<user>/<job name>/<task number>` gibi bir string olabilir ve bu `<IP address>:<port>` ÅŸeklinde Ã§Ã¶zÃ¼mlenir.

Borg ayrÄ±ca job'lara kaynak tahsisinden de sorumludur. Her job'Ä±n gerekli kaynaklarÄ±nÄ± belirtmesi gerekir (Ã¶rneÄŸin, 3 CPU core, 2 GiB RAM). TÃ¼m job'larÄ±n gereksinimlerinin listesini kullanarak, Borg task'larÄ± makineler Ã¼zerinde failure domain'leri de hesaba katan optimal bir ÅŸekilde binpack edebilir (Ã¶rneÄŸin: Borg bir job'Ä±n tÃ¼m task'larÄ±nÄ± aynÄ± rack'te Ã§alÄ±ÅŸtÄ±rmaz, Ã§Ã¼nkÃ¼ bunu yapmak top of rack switch'ini o job iÃ§in single point of failure yapar).

EÄŸer bir task talep ettiÄŸinden daha fazla kaynak kullanmaya Ã§alÄ±ÅŸÄ±rsa, Borg task'Ä± Ã¶ldÃ¼rÃ¼r ve yeniden baÅŸlatÄ±r (yavaÅŸ crashloop yapan bir task genellikle hiÃ§ yeniden baÅŸlatÄ±lmamÄ±ÅŸ bir task'tan daha iyidir).

## Storage

Task'lar makinelerdeki local disk'i scratch pad olarak kullanabilir, ancak kalÄ±cÄ± storage (ve hatta scratch space bile sonunda cluster storage modeline geÃ§ecek) iÃ§in birkaÃ§ cluster storage seÃ§eneÄŸimiz var. Bunlar ikisi de aÃ§Ä±k kaynak cluster filesystem'larÄ± olan Lustre ve Hadoop Distributed File System (HDFS) ile karÅŸÄ±laÅŸtÄ±rÄ±labilir.

Storage katmanÄ±, kullanÄ±cÄ±lara bir cluster iÃ§in mevcut storage'a kolay ve gÃ¼venilir eriÅŸim sunmaktan sorumludur. Åekil 2-3'te gÃ¶sterildiÄŸi gibi, storage'Ä±n birÃ§ok katmanÄ± vardÄ±r:

1. En alt katman _D_ olarak adlandÄ±rÄ±lÄ±r (_disk_ iÃ§in, ancak D hem spinning diskler hem de flash storage kullanÄ±r). D bir clusterdaki neredeyse tÃ¼m makinelerde Ã§alÄ±ÅŸan bir fileserver'dÄ±r. Ancak verilerine eriÅŸmek isteyen kullanÄ±cÄ±lar verilerinin hangi makinede saklandÄ±ÄŸÄ±nÄ± hatÄ±rlamak istemez, iÅŸte bir sonraki katmanÄ±n devreye girdiÄŸi yer burasÄ±dÄ±r.

2. D'nin Ã¼stÃ¼nde _Colossus_ adlÄ± bir katman, olaÄŸan filesystem semantiÄŸi, replikasyon ve ÅŸifreleme sunan cluster Ã§apÄ±nda bir filesystem oluÅŸturur. Colossus, Google File System olan GFS'nin halefidir.

3. Colossus'un Ã¼stÃ¼nde birkaÃ§ database benzeri servis bulunur:
   - **Bigtable** petabyte boyutunda database'leri kaldÄ±rabilen bir NoSQL database sistemidir. Bir Bigtable row key, column key ve timestamp ile indexlenmiÅŸ sparse, distributed, persistent multidimensional sorted map'tir; map'teki her deÄŸer yorumlanmamÄ±ÅŸ byte dizisidir. Bigtable eventually consistent, cross-datacenter replikasyonu destekler.
   - **Spanner** dÃ¼nya Ã§apÄ±nda gerÃ§ek consistency gerektiren kullanÄ±cÄ±lar iÃ§in SQL benzeri bir arayÃ¼z sunar.
   - _Blobstore_ gibi diÄŸer birkaÃ§ database sistemi mevcuttur. Bu seÃ§eneklerin her biri kendi trade-off'larÄ± ile gelir.

## Networking

Google'Ä±n network hardware'Ä± Ã§eÅŸitli ÅŸekillerde kontrol edilir. Daha Ã¶nce tartÄ±ÅŸtÄ±ÄŸÄ±mÄ±z gibi, OpenFlow tabanlÄ± software-defined network kullanÄ±yoruz. "AkÄ±llÄ±" routing hardware'Ä± kullanmak yerine, network boyunca en iyi yollarÄ± Ã¶nceden hesaplayan merkezi (yedekli) controller ile kombinasyon halinde daha ucuz "aptal" switching komponentlerine gÃ¼veniyoruz. Bu nedenle, compute-expensive routing kararlarÄ±nÄ± router'lardan uzaklaÅŸtÄ±rabilir ve basit switching hardware'Ä± kullanabiliriz.

Network bandwidth'inin akÄ±llÄ±ca tahsis edilmesi gerekir. Borg'un bir task'Ä±n kullanabileceÄŸi compute kaynaklarÄ±nÄ± sÄ±nÄ±rlamasÄ± gibi, Bandwidth Enforcer (BwE) ortalama mevcut bandwidth'i maksimize etmek iÃ§in mevcut bandwidth'i yÃ¶netir.

BazÄ± servislerin dÃ¼nya Ã§apÄ±nda daÄŸÄ±tÄ±lmÄ±ÅŸ birden fazla clusterde Ã§alÄ±ÅŸan job'larÄ± vardÄ±r. Global olarak daÄŸÄ±tÄ±lmÄ±ÅŸ servisler iÃ§in latency'yi minimize etmek iÃ§in, kullanÄ±cÄ±larÄ± mevcut kapasiteye sahip en yakÄ±n datacenter'a yÃ¶nlendirmek istiyoruz. _Global Software Load Balancer_ (GSLB) Ã¼Ã§ seviyede load balancing gerÃ§ekleÅŸtirir:

* DNS istekleri iÃ§in geographic load balancing (Ã¶rneÄŸin, _www.google.com_)
* User service seviyesinde load balancing (Ã¶rneÄŸin, YouTube veya Google Maps)
* Remote Procedure Call (RPC) seviyesinde load balancing

Service sahipleri bir servis iÃ§in sembolik bir isim, server'larÄ±n BNS adreslerinin listesi ve her lokasyonda mevcut kapasiteyi (genellikle saniye baÅŸÄ±na sorgu ile Ã¶lÃ§Ã¼lÃ¼r) belirtir. GSLB daha sonra trafiÄŸi BNS adreslerine yÃ¶nlendirir.

# DiÄŸer System Software'larÄ±

Bir datacenterdaki diÄŸer birkaÃ§ komponent de Ã¶nemlidir.

## Lock Service

_Chubby_ lock servisi lock'larÄ± sÃ¼rdÃ¼rmek iÃ§in filesystem benzeri bir API saÄŸlar. Chubby bu lock'larÄ± datacenter lokasyonlarÄ± boyunca yÃ¶netir. Asynchronous Consensus iÃ§in Paxos protokolÃ¼nÃ¼ kullanÄ±r.

Chubby ayrÄ±ca master election'da Ã¶nemli bir rol oynar. Bir servisin gÃ¼venilirlik amaÃ§lÄ± beÅŸ replika job'Ä± Ã§alÄ±ÅŸÄ±yor ancak sadece bir replika gerÃ§ek iÅŸi yapabiliyorsa, Chubby _hangi_ replikanÄ±n devam edebileceÄŸini seÃ§mek iÃ§in kullanÄ±lÄ±r.

Consistent olmasÄ± gereken veriler Chubby'de saklanmaya uygundur. Bu nedenle, BNS, BNS path'leri ile `IP address:port` Ã§iftleri arasÄ±ndaki mapping'i saklamak iÃ§in Chubby kullanÄ±r.

## Monitoring ve Alerting

TÃ¼m servislerin gerektiÄŸi gibi Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olmak istiyoruz. Bu nedenle, _Borgmon_ monitoring programÄ±mÄ±zÄ±n birÃ§ok instance'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±yoruz. Borgmon dÃ¼zenli olarak monitÃ¶rlenen serverlardan metrikleri "scrape" eder. Bu metrikler alerting iÃ§in anÄ±nda kullanÄ±labilir ve ayrÄ±ca tarihi genel bakÄ±ÅŸlarda (Ã¶rneÄŸin, grafikler) kullanÄ±m iÃ§in saklanabilir. Monitoring'i Ã§eÅŸitli ÅŸekillerde kullanabiliriz:

* Akut problemler iÃ§in alerting kurma.
* DavranÄ±ÅŸ karÅŸÄ±laÅŸtÄ±rmasÄ±: bir software gÃ¼ncellemesi serveri daha hÄ±zlÄ± mÄ± yaptÄ±?
* Kaynak tÃ¼ketimi davranÄ±ÅŸÄ±nÄ±n zaman iÃ§inde nasÄ±l evrildiÄŸini inceleme, bu capacity planning iÃ§in esastÄ±r.

# Software AltyapÄ±mÄ±z

Software mimarimiz hardware altyapÄ±mÄ±zdan en verimli ÅŸekilde yararlanmak iÃ§in tasarlanmÄ±ÅŸtÄ±r. KodumÄ±z yoÄŸun ÅŸekilde multithreaded'dir, bu yÃ¼zden bir task kolayca birÃ§ok core kullanabilir. Dashboard'lar, monitoring ve debugging'i kolaylaÅŸtÄ±rmak iÃ§in, her server belirli bir task iÃ§in diagnostik ve istatistik saÄŸlayan bir HTTP server'a sahiptir.

Google'Ä±n tÃ¼m servisleri _Stubby_ adlÄ± Remote Procedure Call (RPC) altyapÄ±sÄ± kullanarak iletiÅŸim kurar; aÃ§Ä±k kaynak versiyonu olan gRPC mevcuttur. SÄ±klÄ±kla yerel programdaki bir subroutine Ã§aÄŸrÄ±sÄ± yapmak gerektiÄŸinde bile RPC Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±r. Bu, daha fazla modÃ¼lerlik gerektiÄŸinde veya bir server'Ä±n codebase'i bÃ¼yÃ¼dÃ¼ÄŸÃ¼nde Ã§aÄŸrÄ±yÄ± farklÄ± bir server'a refactor etmeyi kolaylaÅŸtÄ±rÄ±r. GSLB, externally visible servisleri load balance ettiÄŸi ÅŸekilde RPC'leri de load balance edebilir.

Bir server _frontend_'inden RPC istekleri alÄ±r ve _backend_'ine RPC gÃ¶nderir. Geleneksel terimlerle, frontend client, backend ise server olarak adlandÄ±rÄ±lÄ±r.

Veriler RPC'ye ve RPC'den Apache'nin Thrift'ine benzer olan _protocol buffer_'lar kullanÄ±larak aktarÄ±lÄ±r, genellikle "protobuf" olarak kÄ±saltÄ±lÄ±r. Protocol buffer'lar structured data serialize etmek iÃ§in XML'e gÃ¶re birÃ§ok avantaja sahiptir: kullanÄ±mÄ± daha basit, 3 ila 10 kat daha kÃ¼Ã§Ã¼k, 20 ila 100 kat daha hÄ±zlÄ± ve daha az belirsizdir.

# Development Environment'Ä±mÄ±z

Development velocity Google iÃ§in Ã§ok Ã¶nemlidir, bu yÃ¼zden altyapÄ±mÄ±zdan yararlanmak iÃ§in eksiksiz bir development environment oluÅŸturduk.

Kendi aÃ§Ä±k kaynak repository'leri olan birkaÃ§ grup dÄ±ÅŸÄ±nda (Ã¶rneÄŸin, Android ve Chrome), Google Software Engineer'larÄ± tek bir paylaÅŸÄ±lan repository'den Ã§alÄ±ÅŸÄ±r. Bu workflow'larÄ±mÄ±z iÃ§in birkaÃ§ Ã¶nemli pratik sonucu vardÄ±r:

* Engineer'lar projelerinin dÄ±ÅŸÄ±ndaki bir komponente problem yaÅŸarlarsa, problemi dÃ¼zeltebilir, Ã¶nerilen deÄŸiÅŸiklikleri ("changelist" veya _CL_) sahibine review iÃ§in gÃ¶nderebilir ve CL'yi mainline'a submit edebilir.
* Bir engineer'Ä±n kendi projesindeki kaynak kod deÄŸiÅŸiklikleri review gerektirir. TÃ¼m software submit edilmeden Ã¶nce review edilir.

Software build edildiÄŸinde, build isteÄŸi bir datacenterdaki build server'lara gÃ¶nderilir. BirÃ§ok build server paralel olarak compile edebildiÄŸi iÃ§in bÃ¼yÃ¼k build'ler bile hÄ±zlÄ±ca execute edilir. Bu altyapÄ± ayrÄ±ca continuous testing iÃ§in de kullanÄ±lÄ±r. Bir CL submit edildiÄŸinde her seferinde, o CL'ye doÄŸrudan veya dolaylÄ± olarak baÄŸÄ±mlÄ± olabilecek tÃ¼m software Ã¼zerinde testler Ã§alÄ±ÅŸÄ±r. EÄŸer framework deÄŸiÅŸikliÄŸin sistemin diÄŸer kÄ±sÄ±mlarÄ±nÄ± muhtemelen bozduÄŸunu belirlerse, submit edilen deÄŸiÅŸikliÄŸin sahibini bilgilendirir. BazÄ± projeler push-on-green sistemi kullanÄ±r; burada testleri geÃ§tikten sonra yeni versiyon otomatik olarak production'a push edilir.

# Shakespeare: Ã–rnek Bir Servis

Google production environment'Ä±nda bir servisin hipotetik olarak nasÄ±l deploy edileceÄŸinin bir modelini saÄŸlamak iÃ§in birden fazla Google teknolojisiyle etkileÅŸime giren Ã¶rnek bir servise bakalÄ±m. Diyelim ki Shakespeare'in tÃ¼m eserlerinde belirli bir kelimenin nerede kullanÄ±ldÄ±ÄŸÄ±nÄ± belirlemenizi saÄŸlayan bir servis sunmak istiyoruz.

Bu sistemi iki kÄ±sma ayÄ±rabiliriz:

* Shakespeare'in tÃ¼m metinlerini okuyan, index oluÅŸturan ve index'i Bigtable'a yazan batch komponenti. Bu job'Ä±n sadece bir kez veya Ã§ok seyrek Ã§alÄ±ÅŸmasÄ± gerekir.
* End-user isteklerini yÃ¶neten application frontend'i. Bu job her zaman ayakta durur, Ã§Ã¼nkÃ¼ tÃ¼m zaman dilimlerindeki kullanÄ±cÄ±lar Shakespeare'in kitaplarÄ±nda arama yapmak isteyecektir.

Batch komponenti Ã¼Ã§ aÅŸamadan oluÅŸan bir MapReduce'dur.

Mapping aÅŸamasÄ± Shakespeare'in metinlerini okur ve bunlarÄ± tek kelimelere bÃ¶ler. Bu birden fazla worker tarafÄ±ndan paralel olarak gerÃ§ekleÅŸtirilirse daha hÄ±zlÄ±dÄ±r.

Shuffle aÅŸamasÄ± tuple'larÄ± kelimeye gÃ¶re sÄ±ralar.

Reduce aÅŸamasÄ±nda, (_kelime_, _lokasyon listesi_) tuple'Ä± oluÅŸturulur.

Her tuple Bigtable'da bir row'a yazÄ±lÄ±r, kelime key olarak kullanÄ±lÄ±r.

## Bir Request'in YaÅŸamÄ±

Åekil 2-4 bir kullanÄ±cÄ±nÄ±n request'inin nasÄ±l servis edildiÄŸini gÃ¶sterir: Ã¶nce, kullanÄ±cÄ± browser'Ä±nÄ± _shakespeare.google.com_'a yÃ¶nlendirir. KarÅŸÄ±lÄ±k gelen IP adresini elde etmek iÃ§in, kullanÄ±cÄ±nÄ±n cihazÄ± DNS server'Ä± ile adresi Ã§Ã¶zer (1). Bu request sonuÃ§ta GSLB ile konuÅŸan Google'Ä±n DNS server'Ä±na ulaÅŸÄ±r. GSLB bÃ¶lgeler arasÄ± frontend server'lar arasÄ±ndaki trafik yÃ¼kÃ¼nÃ¼ takip ettiÄŸi iÃ§in, bu kullanÄ±cÄ±ya gÃ¶nderilecek server IP adresini seÃ§er.

Browser bu IP'deki HTTP server'a baÄŸlanÄ±r. Bu server (Google Frontend veya GFE olarak adlandÄ±rÄ±lÄ±r) TCP baÄŸlantÄ±sÄ±nÄ± sonlandÄ±ran bir reverse proxy'dir (2). GFE hangi servisin gerekli olduÄŸunu arar (web search, maps veya bu durumda Shakespeare). Yine GSLB kullanarak, server mevcut bir Shakespeare frontend server'Ä±nÄ± bulur ve o server'a HTTP request'ini iÃ§eren bir RPC gÃ¶nderir (3).

Shakespeare server'Ä± HTTP request'ini analiz eder ve aranacak kelimeyi iÃ§eren bir protobuf oluÅŸturur. Shakespeare frontend server'Ä±nÄ±n ÅŸimdi Shakespeare backend server'Ä±yla iletiÅŸim kurmasÄ± gerekir: frontend server uygun ve yÃ¼klenmemiÅŸ bir backend server'Ä±nÄ±n BNS adresini elde etmek iÃ§in GSLB ile iletiÅŸim kurar (4). O Shakespeare backend server'Ä± ÅŸimdi istenen veriyi elde etmek iÃ§in bir Bigtable server'Ä±yla iletiÅŸim kurar (5).

Cevap reply protobuf'a yazÄ±lÄ±r ve Shakespeare backend server'Ä±na dÃ¶ndÃ¼rÃ¼lÃ¼r. Backend sonuÃ§larÄ± iÃ§eren protobuf'Ä± Shakespeare frontend server'Ä±na verir, o da HTML'i birleÅŸtirir ve cevabÄ± kullanÄ±cÄ±ya dÃ¶ndÃ¼rÃ¼r.

Bu tÃ¼m olay zinciri gÃ¶z aÃ§Ä±p kapayÄ±ncaya kadarâ€”sadece birkaÃ§ yÃ¼z milisaniye iÃ§inde gerÃ§ekleÅŸir! BirÃ§ok hareketli parÃ§a dahil olduÄŸu iÃ§in, birÃ§ok potansiel arÄ±za noktasÄ± vardÄ±r; Ã¶zellikle, arÄ±zalanan bir GSLB bÃ¼yÃ¼k hasar yaratabilir. Ancak, Google'Ä±n titiz test ve dikkatli rollout politikalarÄ±, graceful degradation gibi proaktif hata kurtarma yÃ¶ntemlerimizle birlikte, kullanÄ±cÄ±larÄ±mÄ±zÄ±n beklediÄŸi gÃ¼venilir servisi sunmamÄ±zÄ± saÄŸlar.

## Job ve Data Organizasyonu

Load testing backend server'Ä±mÄ±zÄ±n yaklaÅŸÄ±k 100 sorgu per saniye (QPS) kaldÄ±rabileceÄŸini belirledi. SÄ±nÄ±rlÄ± kullanÄ±cÄ± kÃ¼mesiyle yapÄ±lan denemeler yaklaÅŸÄ±k 3,470 QPS peak load beklememize yol aÃ§tÄ±, bu yÃ¼zden en az 35 task'a ihtiyacÄ±mÄ±z var. Ancak, ÅŸu dÃ¼ÅŸÃ¼nceler job'da en az 37 task'a, yani N+2'ye ihtiyacÄ±mÄ±z olduÄŸu anlamÄ±na gelir:

* GÃ¼ncellemeler sÄ±rasÄ±nda, bir seferde bir task kullanÄ±lamaz durumda olacak, 36 task kalÄ±r.
* Task gÃ¼ncellemesi sÄ±rasÄ±nda makine arÄ±zasÄ± meydana gelebilir, sadece 35 task kalÄ±r, peak load'Ä± karÅŸÄ±lamaya yeterlidir.

KullanÄ±cÄ± trafiÄŸinin daha yakÄ±ndan incelenmesi peak kullanÄ±mÄ±mÄ±zÄ±n global olarak daÄŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir: Kuzey Amerika'dan 1,430 QPS, GÃ¼ney Amerika'dan 290, Avrupa ve Afrika'dan 1,400, Asya ve Avustralya'dan 350. TÃ¼m backend'leri tek bir sitede konumlandÄ±rmak yerine, bunlarÄ± ABD, GÃ¼ney Amerika, Avrupa ve Asya'ya daÄŸÄ±tÄ±yoruz. BÃ¶lge baÅŸÄ±na N+2 redundancy'ye izin vermek ABD'de 17 task, Avrupa'da 16 ve Asya'da 6 task ile sonuÃ§lanÄ±r. Ancak, N+2'nin N+1'e olan overhead'ini dÃ¼ÅŸÃ¼rmek iÃ§in GÃ¼ney Amerika'da (5 yerine) 4 task kullanmaya karar veriyoruz.

Backend'lerin veriyi tutan Bigtable ile iletiÅŸim kurmasÄ± gerektiÄŸi iÃ§in, bu storage elementini de stratejik olarak tasarlamamÄ±z gerekir. Asya'daki bir backend'in ABD'deki Bigtable ile iletiÅŸim kurmasÄ± Ã¶nemli miktarda latency ekler, bu yÃ¼zden Bigtable'Ä± her bÃ¶lgede replika ediyoruz. Bigtable replikasyonu iki ÅŸekilde bize yardÄ±mcÄ± olur: Bigtable server'Ä± arÄ±zalanÄ±rsa resilience saÄŸlar ve veri eriÅŸim latency'sini dÃ¼ÅŸÃ¼rÃ¼r.

**Chapter 2 tamamlandÄ±!** ğŸ‰

Bu bÃ¶lÃ¼mde Google'Ä±n production environment'Ä±nÄ±n detaylarÄ±nÄ±, Borg cluster management sistemini, storage katmanlarÄ±nÄ± (D, Colossus, Bigtable, Spanner), networking altyapÄ±sÄ±nÄ± (Jupiter, B4), monitoring sistemlerini (Borgmon) ve Shakespeare Ã¶rnek servisi Ã¼zerinden bir request'in yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼ Ã¶ÄŸrendik.

Åimdi Chapter 3'e geÃ§ebilirim. HazÄ±r mÄ±sÄ±n? 